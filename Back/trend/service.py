"""
íŠ¸ë Œë“œ ìˆ˜ì§‘ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
"""
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, update
from typing import List, Dict, Any
from loguru import logger
from datetime import datetime

from .models import Keyword, InstagramContent, YouTubeContent, NewsContent
from .schemas import TrendCollectionResponse
from ..clients.apify_client import ApifyService
from ..clients.youtube_client import YouTubeClient
from ..clients.rss_client import RSSClient
from ..clients.crawler_client import CrawlerClient

class TrendService:
    """íŠ¸ë Œë“œ ìˆ˜ì§‘ ë° ë¶„ì„ ì„œë¹„ìŠ¤"""
    
    def __init__(self, db: AsyncSession):
        self.db = db
        self.apify = ApifyService()
        self.youtube = YouTubeClient()
        self.news = RSSClient()
        self.crawler = CrawlerClient()

    async def collect_trending_contents(self, country: str) -> TrendCollectionResponse:
        """
        ì‹¤ì‹œê°„ ì¸ê¸° ì½˜í…ì¸  ì§ì ‘ ìˆ˜ì§‘ (í‚¤ì›Œë“œ ì¤‘ê°„ ë‹¨ê³„ ì—†ìŒ)
        1. YouTube Trending (mostPopular)
        2. Google News Headlines (RSS)
        -> ë°”ë¡œ DB ì €ì¥
        """
        logger.info(f"ğŸ”¥ ì‹¤ì‹œê°„ ì¸ê¸° ì½˜í…ì¸  ìˆ˜ì§‘ ì‹œì‘: {country}")
        
        # ë”ë¯¸ í‚¤ì›Œë“œ ìƒì„± (FK ìš”êµ¬ì‚¬í•­ ì¶©ì¡±)
        today = datetime.now().strftime("%Y%m%d")
        dummy_keyword = f"Trending_{country}_{today}"
        
        # ë”ë¯¸ í‚¤ì›Œë“œ DB ì €ì¥
        keyword_obj = Keyword(
            keyword=dummy_keyword,
            country=country,
            trend_volume=0,
            rank=0
        )
        self.db.add(keyword_obj)
        await self.db.flush()  # ID í™•ë³´
        keyword_id = keyword_obj.id
        
        # 1. YouTube Trending ìˆ˜ì§‘
        youtube_count = 0
        videos = await self.youtube.get_trending_videos(country, max_results=20)
        if videos:
            await self._save_youtube_contents(keyword_id, country, videos)
            youtube_count = len(videos)
            logger.info(f"âœ… YouTube Trending: {youtube_count}ê°œ")
        
        # 2. Google News RSS ìˆ˜ì§‘
        news_count = 0
        articles = await self.crawler._fetch_google_news_rss(country)
        if articles:
            # articlesëŠ” ì´ë¯¸ Dict í˜•íƒœ (keyword, country, rank í¬í•¨)
            # ìš°ë¦¬ëŠ” titleë§Œ í•„ìš”í•˜ë¯€ë¡œ ë³€í™˜
            news_list = []
            for article in articles:
                news_list.append({
                    'title': article['keyword'],  # ë‰´ìŠ¤ ì œëª©ì„ 'keyword' í•„ë“œì—ì„œ ê°€ì ¸ì˜´
                    'source': 'Google News',
                    'description': '',
                    'url': '',  # RSS ìˆ˜ì§‘ ì‹œ URLì´ ì—†ì„ ìˆ˜ ìˆìŒ
                    'published_at': datetime.now().isoformat()
                })
            
            await self._save_news_contents(keyword_id, country, news_list)
            news_count = len(news_list)
            logger.info(f"âœ… Google News: {news_count}ê°œ")
        
        # ì§‘ê³„ ì—…ë°ì´íŠ¸
        await self._update_keyword_aggregates(keyword_id)
        await self.db.commit()
        
        total = youtube_count + news_count
        logger.info(f"ğŸ‰ ì‹¤ì‹œê°„ ì½˜í…ì¸  ìˆ˜ì§‘ ì™„ë£Œ: {total}ê°œ")
        
        return TrendCollectionResponse(
            success=True,
            message=f"ì‹¤ì‹œê°„ ì¸ê¸° ì½˜í…ì¸  {total}ê°œ ìˆ˜ì§‘ ì™„ë£Œ",
            keywords_count=total
        )

    # ===== Private Helper Methods =====
    
    async def _save_youtube_contents(self, keyword_id: int, country: str, videos: List[Dict[str, Any]]):
        """ìœ íŠœë¸Œ ì½˜í…ì¸  ì €ì¥"""
        for video in videos:
            stmt = select(YouTubeContent).where(YouTubeContent.video_id == video["video_id"])
            result = await self.db.execute(stmt)
            existing = result.scalar_one_or_none()
            
            if not existing:
                content = YouTubeContent(
                    keyword_id=keyword_id,
                    keyword_country=country,
                    **video
                )
                self.db.add(content)
        
        await self.db.commit()
    
    async def _save_news_contents(self, keyword_id: int, country: str, articles: List[Dict[str, Any]]):
        """ë‰´ìŠ¤ ì½˜í…ì¸  ì €ì¥"""
        for article in articles:
            # URLì´ ì—†ìœ¼ë©´ ì¤‘ë³µ ì²´í¬ ìƒëµí•˜ê³  ê·¸ëƒ¥ ì €ì¥
            if article.get('url'):
                stmt = select(NewsContent).where(NewsContent.url == article["url"])
                result = await self.db.execute(stmt)
                existing = result.scalar_one_or_none()
                if existing:
                    continue
            
            content = NewsContent(
                keyword_id=keyword_id,
                keyword_country=country,
                **article
            )
            self.db.add(content)
        
        await self.db.commit()
    
    async def _update_keyword_aggregates(self, keyword_id: int):
        """í‚¤ì›Œë“œë³„ ì§‘ê³„ ì—…ë°ì´íŠ¸"""
        youtube_stmt = select(func.count()).select_from(YouTubeContent).where(YouTubeContent.keyword_id == keyword_id)
        news_stmt = select(func.count()).select_from(NewsContent).where(NewsContent.keyword_id == keyword_id)
        
        youtube_count = await self.db.scalar(youtube_stmt) or 0
        news_count = await self.db.scalar(news_stmt) or 0
        
        score = (youtube_count * 1.5) + (news_count * 1)
        
        update_stmt = (
            update(Keyword)
            .where(Keyword.id == keyword_id)
            .values(
                youtube_videos=youtube_count,
                news_count=news_count,
                score=score
            )
        )
        await self.db.execute(update_stmt)
        await self.db.commit()

    async def get_keyword_contents(self, keyword_id: int) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ê´€ë ¨ ì½˜í…ì¸  ìƒì„¸ ì¡°íšŒ"""
        keyword = await self.db.get(Keyword, keyword_id)
        if not keyword:
            return None

        stmt_news = select(NewsContent).where(NewsContent.keyword_id == keyword_id).limit(20)
        news_res = await self.db.execute(stmt_news)
        news_list = news_res.scalars().all()
        
        stmt_youtube = select(YouTubeContent).where(YouTubeContent.keyword_id == keyword_id).limit(10)
        yt_res = await self.db.execute(stmt_youtube)
        yt_list = yt_res.scalars().all()
        
        return {
            "keyword": keyword,
            "news": [{"title": n.title, "url": n.url, "source": n.source, "published_at": str(n.published_at)} for n in news_list],
            "youtube": [{"title": y.title, "url": y.url, "channel": y.channel, "views": y.views} for y in yt_list]
        }
