"""
íŠ¸ë Œë“œ ìˆ˜ì§‘ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
"""
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, update
from typing import List, Dict, Any
from loguru import logger
from datetime import datetime
import asyncio

from .models import Keyword, InstagramContent, YouTubeContent, NewsContent
from .schemas import TrendCollectionResponse
from ..clients.apify_client import ApifyService
from ..clients.youtube_client import YouTubeClient
from ..clients.rss_client import RSSClient
from ..clients.crawler_client import CrawlerClient

class TrendService:
    """íŠ¸ë Œë“œ ìˆ˜ì§‘ ë° ë¶„ì„ ì„œë¹„ìŠ¤"""
    
    def __init__(self, db: AsyncSession):
        self.db = db
        self.apify = ApifyService()
        self.youtube = YouTubeClient()
        self.news = RSSClient()
        self.crawler = CrawlerClient()

    async def collect_trending_contents(self, country: str) -> TrendCollectionResponse:
        """
        ì‹¤ì‹œê°„ ì¸ê¸° ì½˜í…ì¸  ì§ì ‘ ìˆ˜ì§‘ (í‚¤ì›Œë“œ ì¤‘ê°„ ë‹¨ê³„ ì—†ìŒ)
        1. YouTube Trending (mostPopular)
        2. Google News Headlines (RSS)
        -> ë°”ë¡œ DB ì €ì¥
        """
        logger.info(f"ğŸ”¥ ì‹¤ì‹œê°„ ì¸ê¸° ì½˜í…ì¸  ìˆ˜ì§‘ ì‹œì‘: {country}")
        
        # ë”ë¯¸ í‚¤ì›Œë“œ ìƒì„± (FK ìš”êµ¬ì‚¬í•­ ì¶©ì¡±)
        today = datetime.now().strftime("%Y%m%d")
        dummy_keyword = f"Trending_{country}_{today}"
        
        # ë”ë¯¸ í‚¤ì›Œë“œ ì¤‘ë³µ ì²´í¬
        stmt = select(Keyword).where(Keyword.keyword == dummy_keyword).order_by(Keyword.id.desc())
        result = await self.db.execute(stmt)
        keyword_obj = result.scalars().first()
        
        if not keyword_obj:
            keyword_obj = Keyword(
                keyword=dummy_keyword,
                country=country,
                trend_volume=0,
                rank=0
            )
            self.db.add(keyword_obj)
            await self.db.flush()  # ID í™•ë³´
        
        keyword_id = keyword_obj.id
        
        # 1. YouTube Trending ìˆ˜ì§‘
        youtube_count = 0
        videos = await self.youtube.get_trending_videos(country, max_results=20)
        
        # [Plan B] í•œêµ­ì¸ë° Trendingì´ 0ê°œë©´ -> ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ë¡œ ì˜ìƒ ê²€ìƒ‰
        if not videos and country == 'KR':
            logger.warning("âš ï¸ YouTube Trending 0ê°œ -> ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ë¡œ ëŒ€ì²´ ìˆ˜ì§‘ ì‹œë„")
            try:
                loop = asyncio.get_event_loop()
                signal_keywords = await loop.run_in_executor(None, self.crawler._crawl_signal_bz)
                if signal_keywords:
                    top_keyword = signal_keywords[0]['keyword']
                    logger.info(f"ğŸ” ëŒ€ì²´ ê²€ìƒ‰ì–´: {top_keyword}")
                    videos = await self.youtube.search_videos(top_keyword, max_results=10)
            except Exception as e:
                logger.error(f"Plan B ì‹¤íŒ¨: {e}")

        if videos:
            await self._save_youtube_contents(keyword_id, country, videos)
            youtube_count = len(videos)
            logger.info(f"âœ… YouTube Trending: {youtube_count}ê°œ")
        
        # 2. Google News RSS ìˆ˜ì§‘
        news_count = 0
        loop = asyncio.get_event_loop()
        articles = await loop.run_in_executor(None, self.crawler._fetch_google_news_rss, country)
        
        # 3. (í•œêµ­ ì „ìš©) Signal.bz ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ìˆ˜ì§‘
        if country == 'KR':
            try:
                signal_keywords = await loop.run_in_executor(None, self.crawler._crawl_signal_bz)
                if signal_keywords:
                    logger.info(f"âœ… Signal.bz ì¶”ê°€: {len(signal_keywords)}ê°œ")
                    # ì‹¤ê²€ì„ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì•ë‹¨ì— ì¶”ê°€
                    for item in signal_keywords:
                        articles.insert(0, {
                            'keyword': f"ğŸ”¥ {item['keyword']}", # ê°•ì¡° í‘œì‹œ
                            'url': '', # ì‹¤ê²€ì€ URL ì—†ìŒ (Google ê²€ìƒ‰ ë§í¬ë¥¼ ë§Œë“¤ì–´ì¤„ ìˆ˜ë„ ìˆìŒ)
                            'published_at': datetime.now().isoformat()
                        })
            except Exception as e:
                logger.warning(f"Signal.bz ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        if articles:
            # articlesëŠ” ì´ë¯¸ Dict í˜•íƒœ (keyword, country, rank í¬í•¨)
            # ìš°ë¦¬ëŠ” titleë§Œ í•„ìš”í•˜ë¯€ë¡œ ë³€í™˜
            news_list = []
            for article in articles:
                news_list.append({
                    'title': article['keyword'],  # ë‰´ìŠ¤ ì œëª©
                    'source': 'Google News' if 'keyword' in article and 'ğŸ”¥' not in article['keyword'] else 'ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´',
                    'description': '',
                    'url': article.get('url', ''),
                    # ì‹¤ê²€ì˜ ê²½ìš° êµ¬ê¸€ ê²€ìƒ‰ URL ìƒì„±
                    'url': article.get('url') or (f"https://www.google.com/search?q={article['keyword'].replace('ğŸ”¥ ', '')}" if 'ğŸ”¥' in article['keyword'] else ''),
                    'published_at':  article.get('published_at') or datetime.now().isoformat()
                })
            
            await self._save_news_contents(keyword_id, country, news_list)
            news_count = len(news_list)
            logger.info(f"âœ… News + Signal: {news_count}ê°œ")
        
        # ì§‘ê³„ ì—…ë°ì´íŠ¸
        await self._update_keyword_aggregates(keyword_id)
        await self.db.commit()
        
        total = youtube_count + news_count
        logger.info(f"ğŸ‰ ì‹¤ì‹œê°„ ì½˜í…ì¸  ìˆ˜ì§‘ ì™„ë£Œ: {total}ê°œ")
        
        return TrendCollectionResponse(
            success=True,
            message=f"ì‹¤ì‹œê°„ ì¸ê¸° ì½˜í…ì¸  {total}ê°œ ìˆ˜ì§‘ ì™„ë£Œ",
            keywords_count=total
        )

    # ===== Private Helper Methods =====
    
    async def _save_youtube_contents(self, keyword_id: int, country: str, videos: List[Dict[str, Any]]):
        """ìœ íŠœë¸Œ ì½˜í…ì¸  ì €ì¥"""
        logger.info(f"ğŸ¬ YouTube ì €ì¥ ì‹œì‘: keyword_id={keyword_id}, ì˜ìƒ ìˆ˜={len(videos)}")
        saved_count = 0
        skipped_count = 0
        
        for idx, video in enumerate(videos):
            try:
                video_id = video.get("video_id")
                title = video.get("title", "ì œëª© ì—†ìŒ")[:50]
                logger.info(f"  [{idx+1}/{len(videos)}] ì²˜ë¦¬ ì¤‘: {title}...")
                
                # ì¤‘ë³µ ì²´í¬
                stmt = select(YouTubeContent).where(YouTubeContent.video_id == video_id)
                result = await self.db.execute(stmt)
                existing = result.scalar_one_or_none()
                
                if existing:
                    # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì˜ìƒì´ë©´, í˜„ì¬ í‚¤ì›Œë“œ(ìµœì‹ ) ì†Œì†ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                    logger.info(f"    â™»ï¸ ì¤‘ë³µ ì˜ìƒ -> ìµœì‹  í‚¤ì›Œë“œ({keyword_id})ë¡œ ì†Œì† ë³€ê²½ (video_id={video_id})")
                    existing.keyword_id = keyword_id
                    existing.collected_at = func.now() # ìˆ˜ì§‘ ì‹œê°ë„ ê°±ì‹ 
                    # ì¡°íšŒìˆ˜ ë“± ìµœì‹  ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
                    existing.views = video.get("views", existing.views)
                    existing.likes = video.get("likes", existing.likes)
                    
                    skipped_count += 1
                    saved_count += 1 # í™”ë©´ì— ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì¹´ìš´íŠ¸ í¬í•¨
                    # commitì€ ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆì— í•¨ (SQLAlchemy ê°ì²´ ë³€ê²½ ê°ì§€)
                    continue
                
                # ì €ì¥ (ì‹ ê·œ)
                content = YouTubeContent(
                    keyword_id=keyword_id,
                    keyword_country=country,
                    **video
                )
                self.db.add(content)
                saved_count += 1
                logger.info(f"    âœ… ì €ì¥ ì˜ˆì • (ëˆ„ì  {saved_count}ê°œ)")
                
            except Exception as e:
                logger.error(f"    âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
                continue
        
        await self.db.commit()
        logger.info(f"ğŸ¬ YouTube ì €ì¥ ì™„ë£Œ: ì‹ ê·œ {saved_count}ê°œ, ì¤‘ë³µ {skipped_count}ê°œ, ì´ ì»¤ë°‹ë¨")
    
    async def _save_news_contents(self, keyword_id: int, country: str, articles: List[Dict[str, Any]]):
        """ë‰´ìŠ¤ ì½˜í…ì¸  ì €ì¥"""
        for article in articles:
            url = article.get('url', '')
            if not url:
                continue

            # URL ì¤‘ë³µ ì²´í¬
            stmt = select(NewsContent).where(NewsContent.url == url)
            result = await self.db.execute(stmt)
            existing = result.scalar_one_or_none()
            if existing:
                # ì¤‘ë³µ ë‰´ìŠ¤ -> ìµœì‹  í‚¤ì›Œë“œë¡œ ì†Œì† ì—…ë°ì´íŠ¸
                existing.keyword_id = keyword_id
                existing.collected_at = func.now()
                continue
            
            content = NewsContent(
                keyword_id=keyword_id,
                keyword_country=country,
                **article
            )
            self.db.add(content)
        
        await self.db.commit()
    
    async def _update_keyword_aggregates(self, keyword_id: int):
        """í‚¤ì›Œë“œë³„ ì§‘ê³„ ì—…ë°ì´íŠ¸"""
        youtube_stmt = select(func.count()).select_from(YouTubeContent).where(YouTubeContent.keyword_id == keyword_id)
        news_stmt = select(func.count()).select_from(NewsContent).where(NewsContent.keyword_id == keyword_id)
        
        youtube_count = await self.db.scalar(youtube_stmt) or 0
        news_count = await self.db.scalar(news_stmt) or 0
        
        score = (youtube_count * 1.5) + (news_count * 1)
        
        update_stmt = (
            update(Keyword)
            .where(Keyword.id == keyword_id)
            .values(
                youtube_videos=youtube_count,
                news_count=news_count,
                score=score
            )
        )
        await self.db.execute(update_stmt)
        await self.db.commit()

    async def get_keyword_contents(self, keyword_id: int) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ê´€ë ¨ ì½˜í…ì¸  ìƒì„¸ ì¡°íšŒ"""
        keyword = await self.db.get(Keyword, keyword_id)
        if not keyword:
            return None

        stmt_news = select(NewsContent).where(NewsContent.keyword_id == keyword_id).limit(20)
        news_res = await self.db.execute(stmt_news)
        news_list = news_res.scalars().all()
        
        stmt_youtube = select(YouTubeContent).where(YouTubeContent.keyword_id == keyword_id).limit(10)
        yt_res = await self.db.execute(stmt_youtube)
        yt_list = yt_res.scalars().all()
        
        return {
            "keyword": keyword,
            "news": [{"title": n.title, "url": n.url, "source": n.source, "published_at": str(n.published_at)} for n in news_list],
            "youtube": [{"title": y.title, "url": y.url, "channel": y.channel, "views": y.views} for y in yt_list]
        }
